# -*- coding: utf-8 -*-
"""Save.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-D27UtF50yrNuXMMp8sxBzlUV39PIcI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score


data = pd.read_csv('AI_ML_1.csv')


data = data.replace({',': ''}, regex=True)
data = data.apply(pd.to_numeric, errors='coerce')


X = data.iloc[:, :10].values
y = data.iloc[:, 10].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.08, random_state=42
)


rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)


y_pred = rf.predict(X_test)


mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Test MSE: {mse:.4f}")
print(f"R² Score: {r2:.4f}")


plt.scatter(y_test, y_pred, alpha=1, label='Predicted vs. True')

plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')

plt.xlabel('True Values', fontsize=14, fontweight='bold')
plt.ylabel('Predicted Values', fontsize=14, fontweight='bold')

plt.xticks(fontsize=12, fontweight='bold')
plt.yticks(fontsize=12, fontweight='bold')
plt.legend(fontsize=12, loc='upper left', frameon=True, prop={'weight': 'bold'})
plt.gca().spines['top'].set_linewidth(2)
plt.gca().spines['right'].set_linewidth(2)
plt.gca().spines['bottom'].set_linewidth(2)
plt.gca().spines['left'].set_linewidth(2)
plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score


data = pd.read_csv('AI_ML_1.csv')


data = data.replace({',': ''}, regex=True)
data = data.apply(pd.to_numeric, errors='coerce')


X = data.iloc[:, :10].values
y = data.iloc[:, 10].values


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.08, random_state=42
)


xgb_reg = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb_reg.fit(X_train, y_train)


y_pred = xgb_reg.predict(X_test)


mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Test MSE: {mse:.4f}")
print(f"R² Score: {r2:.4f}")


plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=1, label='Predicted vs. True')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')

plt.xlabel('True Values', fontsize=12, fontweight='bold')
plt.ylabel('Predicted Values', fontsize=12, fontweight='bold')
plt.xticks(fontsize=12, fontweight='bold')
plt.yticks(fontsize=12, fontweight='bold')
plt.legend(fontsize=12, loc='upper left', frameon=True, prop={'weight': 'bold'})
plt.gca().spines['top'].set_linewidth(2)
plt.gca().spines['right'].set_linewidth(2)
plt.gca().spines['bottom'].set_linewidth(2)
plt.gca().spines['left'].set_linewidth(2)
plt.show()





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import lightgbm as lgb

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score


data = pd.read_csv('AI_ML_1.csv')


data = data.replace({',': ''}, regex=True)
data = data.apply(pd.to_numeric, errors='coerce')


X = data.iloc[:, :10].values
y = data.iloc[:, 10].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.08, random_state=42
)


rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)


xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42)
xgb_reg.fit(X_train, y_train)
y_pred_xgb = xgb_reg.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

lgb_reg = lgb.LGBMRegressor(n_estimators=500, learning_rate=0.05, max_depth=10, num_leaves=60, min_child_samples=5, subsample=0.9, colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=0.1, random_state=42)
lgb_reg.fit(X_train, y_train)
y_pred_lgb = lgb_reg.predict(X_test)
mse_lgb = mean_squared_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)


models = ["Random Forest", "XGBoost", "LightGBM"]
mse_values = [mse_rf, mse_xgb, mse_lgb]
r2_values = [r2_rf, r2_xgb, r2_lgb]


fig, ax = plt.subplots(1, 2, figsize=(14, 5))


ax[0].bar(models, mse_values, color=["blue", "orange", "green"], alpha=0.7)
ax[0].set_ylabel("Mean Squared Error", fontsize=12, fontweight='bold')
ax[0].set_title("MSE Comparison", fontsize=14, fontweight='bold')
ax[0].tick_params(axis='x', labelsize=12)
ax[0].grid(axis='y', linestyle='--', alpha=0.6)


ax[1].bar(models, r2_values, color=["blue", "orange", "green"], alpha=0.7)
ax[1].set_ylabel("R² Score", fontsize=12, fontweight='bold')
ax[1].set_title("R² Score Comparison", fontsize=14, fontweight='bold')
ax[1].tick_params(axis='x', labelsize=12)
ax[1].grid(axis='y', linestyle='--', alpha=0.6)
ax[1].set_ylim(0, 1)
plt.tight_layout()
plt.show()



